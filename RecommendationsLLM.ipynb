{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling\n",
        "import json\n",
        "\n"
      ],
      "metadata": {
        "id": "x1CUMQaG-p_O"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# method to load the data from json files\n",
        "def load_exercises(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    text = \"\"\n",
        "    for body_part, content in data.items():\n",
        "        text += f\"{body_part}:\\n\"\n",
        "        for exercise in content['exercises']:\n",
        "            text += f\"- {exercise['name']}: {exercise['explanation']}\\n\"\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "Wag7B4AnFJY5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text = load_exercises('/content/training_data.json')\n",
        "test_text = load_exercises('/content/test_data.json')"
      ],
      "metadata": {
        "id": "ry8AqKlvFKLH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# saving the preprocessed\n",
        "with open('/content/train.txt', 'w') as f:\n",
        "    f.write(train_text)\n",
        "\n",
        "with open('/content/test.txt', 'w') as f:\n",
        "    f.write(test_text)\n"
      ],
      "metadata": {
        "id": "dj7jePPhIu1o"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# loading tokenizer and the gpt2 model used\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n"
      ],
      "metadata": {
        "id": "_ed755pKE33O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# dataset preparation\n",
        "def load_dataset(file_path, tokenizer):\n",
        "    return TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=file_path,\n",
        "        block_size=128,\n",
        "    )\n",
        "\n",
        "\n",
        "train_dataset = load_dataset('/content/train.txt', tokenizer)\n",
        "test_dataset = load_dataset('/content/test.txt', tokenizer)\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zS8ErPEE5eN",
        "outputId": "2db822e7-19c0-4eec-c1fd-37b2c9b8acb9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setting up the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/results',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "# initializing trainer for data input\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "6o5ALZnQE7EM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# llm model training process\n",
        "trainer.train()\n",
        "trainer.save_model('/content/trained_model')\n",
        "tokenizer.save_pretrained('/content/trained_model')  # Save the tokenizer as well\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "B3W-x8qwE8Ds",
        "outputId": "f6912ddc-3478-4b96-b476-5c46bcdaa43d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [15/15 01:56, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/trained_model/tokenizer_config.json',\n",
              " '/content/trained_model/special_tokens_map.json',\n",
              " '/content/trained_model/vocab.json',\n",
              " '/content/trained_model/merges.txt',\n",
              " '/content/trained_model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# path specification to save the model\n",
        "model_path = '/content/trained_model'\n",
        "\n",
        "# loading fine tuned model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "\n",
        "#  setting pad token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "#  example user input\n",
        "input_text = (\n",
        "    \"I have lower back pain and I'm looking for exercises to help relieve it. \"\n",
        "    \"Please suggest some effective exercises along with brief explanations.\\n\\n\"\n",
        "    \"Exercises:\\n\"\n",
        ")\n",
        "\n",
        "# input encoding and attention mask generation\n",
        "#inputs = tokenizer.encode(input_text, return_tensors='pt')\n",
        "inputs = tokenizer(input_text, return_tensors='pt').input_ids.to(model.device)\n",
        "\n",
        "attention_mask = inputs.ne(tokenizer.pad_token_id).long()\n",
        "\n",
        "# generating suggestions with attention mask and adjusted parameters\n",
        "outputs = model.generate(\n",
        "    inputs,\n",
        "    attention_mask=attention_mask,\n",
        "    max_length=250,\n",
        "    num_return_sequences=1,\n",
        "    #temperature=0.9,\n",
        "    do_sample = False,\n",
        "    #top_p=0.9,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")[0]\n",
        "\n",
        "print(outputs)\n",
        "int_answer = int(inputs.shape[-1])\n",
        "suggestions = tokenizer.decode(outputs[int_answer:], skip_special_tokens=True)\n",
        "\n",
        "print(suggestions)\n",
        "\n",
        "print(f'### User Input:\\n{input_text}\\n\\n### Assistant Output:\\n{suggestions}')\n"
      ],
      "metadata": {
        "id": "gdODvd08n5ei"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}