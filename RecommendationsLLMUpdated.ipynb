{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
        "\n",
        "# Load the JSON file from the specified path\n",
        "file_path = '/content/training_data.json'\n",
        "with open(file_path) as file:\n",
        "    data = json.load(file)\n"
      ],
      "metadata": {
        "id": "OoeknRdthhoY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Prepare the data for training\n",
        "exercises = []\n",
        "for body_part, content in data.items():\n",
        "    for exercise in content['exercises']:\n",
        "        exercises.append({\n",
        "            \"body_part\": body_part,\n",
        "            \"exercise_name\": exercise[\"name\"],\n",
        "            \"explanation\": exercise[\"explanation\"]\n",
        "        })\n"
      ],
      "metadata": {
        "id": "cCuPg7nmiwph"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize the tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Add a padding token if it doesn't exist\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "model.resize_token_embeddings(len(tokenizer))  # Resize model embeddings to account for new tokens\n"
      ],
      "metadata": {
        "id": "APOvEW4Eix-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_data(exercise):\n",
        "    tokenized_explanation = tokenizer(exercise['explanation'], padding='max_length', truncation=True, max_length=512)\n",
        "    tokenized_explanation['labels'] = tokenized_explanation['input_ids'].copy()  # Use input_ids as labels\n",
        "    return tokenized_explanation\n",
        "\n",
        "\n",
        "\n",
        "tokenized_data = [tokenize_data(ex) for ex in exercises]\n"
      ],
      "metadata": {
        "id": "aolGR2b5izRq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ExerciseDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tokenized_data):\n",
        "        self.input_ids = [td['input_ids'] for td in tokenized_data]\n",
        "        self.attention_mask = [td['attention_mask'] for td in tokenized_data]\n",
        "        self.labels = [td['labels'] for td in tokenized_data]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "dataset = ExerciseDataset(tokenized_data)\n",
        "\n"
      ],
      "metadata": {
        "id": "e94RTdDVi1UF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        ")\n"
      ],
      "metadata": {
        "id": "EaNMVuDLjr5h"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "hYMZHhRwi2wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r6dYfIvFi4QC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}